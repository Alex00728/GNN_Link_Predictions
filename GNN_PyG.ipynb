{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajo 3 Grafos para Data Science - Parte Práctica (Código)\n",
    "Integrantes: Álex Álvarez, Alfonso Tobar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de libraries a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is Set\n"
     ]
    }
   ],
   "source": [
    "# Importamos las libraries necesarias\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GraphConv, SAGEConv, global_mean_pool\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print('Please Activate GPU Accelerator if available')\n",
    "else:\n",
    "    print('Everything is Set')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de los 5 modelos a utilizar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFNN_1HL: red neuronal feed-forward con una capa escondida y función de activación ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN_1HL(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, num_classes):\n",
    "        super(FFNN_1HL, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_features, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        \n",
    "        z1 = self.layer1(x)\n",
    "        h1 = self.act1(z1)\n",
    "        z2 = self.layer2(h1)\n",
    "        return z2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN_1HL_GraphConv: red neuronal de grafos con una capa escondida, funciones de activación ReLU y suma como agregación sobre vecinos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_1HL_GraphConv(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNN_1HL_GraphConv, self).__init__()\n",
    "        self.layer1 = GraphConv(input_dim, hidden_dim)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = GraphConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        z1 = self.layer1(x, edge_index)\n",
    "        h1 = self.act1(z1)\n",
    "        z2 = self.layer2(h1, edge_index)\n",
    "        return z2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN_2HL_GraphConv: red neuronal de grafos con dos capas escondidas, funciones de activación ReLU y suma como agregación sobre vecinos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_2HL_GraphConv(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(GNN_2HL_GraphConv, self).__init__()\n",
    "        self.layer1 = GraphConv(input_dim, hidden_dim1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = GraphConv(hidden_dim1, hidden_dim2)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = GraphConv(hidden_dim2, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        z1 = self.layer1(x, edge_index)\n",
    "        h1 = self.act1(z1)\n",
    "        z2 = self.layer2(h1, edge_index)\n",
    "        h2 = self.act2(z2)\n",
    "        z3 = self.layer3(h2, edge_index)\n",
    "        return z3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN_1HL_SAGEConv: red neuronal de grafos con una capa escondida, funciones de activación ReLU y media como agregación sobre vecinos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_1HL_SAGEConv(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GNN_1HL_SAGEConv, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        z1 = self.conv1(x, edge_index)\n",
    "        h1 = self.act1(z1)\n",
    "        z2 = self.conv2(h1, edge_index)\n",
    "        return z2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN_2HL_SAGEConv: red neuronal de grafos con dos capas escondidas, funciones de activación ReLU y media como agregación sobre vecinos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_2HL_SAGEConv(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels1, hidden_channels2, out_channels):\n",
    "        super(GNN_2HL_SAGEConv, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = SAGEConv(hidden_channels1, hidden_channels2)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.conv3 = SAGEConv(hidden_channels2, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        z1 = self.conv1(x, edge_index)\n",
    "        h1 = self.act1(z1)\n",
    "        z2 = self.conv2(h1, edge_index)\n",
    "        h2 = self.act2(z2)\n",
    "        z3 = self.conv3(h2, edge_index)\n",
    "        return z3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para evaluar los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, data, num_epochs = 10, num_trials = 5):\n",
    "    # Inicializamos una lista para almacenar los resultados\n",
    "    results = []\n",
    "    # Iteramos num_trials veces sobre cada modelo a evaluar\n",
    "    for trial in range(num_trials):\n",
    "    # Iteramos sobre cada modelo a evaluar\n",
    "        for model in models:\n",
    "            start_time = time.time()\n",
    "            # Inicializamos el optimizador Adam con los parámetros del modelo\n",
    "            optimizer = torch.optim.Adam(model.parameters())\n",
    "            # Inicializamos la función de pérdida como CrossEntropyLoss\n",
    "            loss_function = torch.nn.CrossEntropyLoss()\n",
    "            # Iteramos sobre las épocas de entrenamiento\n",
    "            for epoch in range(num_epochs):\n",
    "                # Calculamos la salida del modelo a partir del conjunto de entrenamiento\n",
    "                out = model(data)\n",
    "                # Calculamos la pérdida utilizando la función de pérdida\n",
    "                loss = loss_function(out[data.train_mask], data.y[data.train_mask])\n",
    "                # Realizamos el backpropagation para actualizar los parámetros del modelo\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Resetear los gradientes\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Calculamos la precisión del modelo en el conjunto de testeo\n",
    "            _, pred = torch.max(model(data), dim=1)\n",
    "            correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "            accuracy = correct / int(data.test_mask.sum())\n",
    "            elapsed_time = time.time() - start_time\n",
    "            # Guardamos resultados en tuplas (accuracy, elapsed_time)\n",
    "            results.append((accuracy, elapsed_time))\n",
    "    # Calculamos los promedios de accuracy por cada modelo\n",
    "    avg_accuracy = [sum([result[0] for result in results[i::len(models)]])/num_trials for i in range(len(models))]\n",
    "    # Calculamos los promedios de tiempo por cada modelo\n",
    "    avg_time = [sum([result[1] for result in results[i::len(models)]])/num_trials for i in range(len(models))]\n",
    "    # Retornamos los resultados\n",
    "    return list(zip(avg_accuracy, avg_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para setear datasets y número de nodos en los modelos:\n",
    "0 para Cora, 1 para Citeseer, 2 para Pubmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_data_nodes(number = 0, batch_size= 32, shuffle=True, HLnodes = 100):\n",
    "    # Nombres de los datasets\n",
    "    datasets = ['Cora', 'Citeseer', 'Pubmed']\n",
    "    # Importamos el dataset\n",
    "    dataset = Planetoid(root='./data', name=datasets[number])\n",
    "    data = dataset[0]\n",
    "    # Guardamos el nombre del dataset seleccionado\n",
    "    dataset_name = datasets[number]\n",
    "    # Data loader (no es necesario para la implementación actual)\n",
    "    data_loader = DataLoader(dataset[0], batch_size=batch_size, shuffle=shuffle)\n",
    "    # Definimos los modelos con la cantidad de nodos en en cada layer\n",
    "    models = [FFNN_1HL(dataset.num_node_features, HLnodes, dataset.num_classes),\n",
    "                GNN_1HL_GraphConv(dataset.num_node_features, HLnodes, dataset.num_classes),\n",
    "                GNN_2HL_GraphConv(dataset.num_node_features, HLnodes, HLnodes, dataset.num_classes),\n",
    "                GNN_1HL_SAGEConv(dataset.num_node_features, HLnodes, dataset.num_classes),\n",
    "                GNN_2HL_SAGEConv(dataset.num_node_features, HLnodes, HLnodes, dataset.num_classes)]\n",
    "    # Retornamos los resultados\n",
    "\n",
    "    return dataset, data_loader, data, dataset_name, models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setear dataset y nodos: (Usar para evaluar modelos en un único dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegimos el dataset 0, 1 o 2 para Cora, Citeseer o Pubmed respectivamente\n",
    "# (Usar para evaluar modelos en un único dataset)\n",
    "dataset, data_loader, data, dataset_name, models = set_data_nodes(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar los modelos en el dataset escogido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name:  Cora\n",
      "FFNN_1HL Accuracy: 0.1270 Time: 0.0270\n",
      "GNN_1HL_GraphConv Accuracy: 0.3040 Time: 0.0971\n",
      "GNN_2HL_GraphConv Accuracy: 0.4630 Time: 0.1200\n",
      "GNN_1HL_SAGEConv Accuracy: 0.1280 Time: 0.1031\n",
      "GNN_2HL_SAGEConv Accuracy: 0.1810 Time: 0.1230\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos los modelos en uno de los datasets\n",
    "# (Usar para evaluar modelos en un único dataset con cierto número de épocas e intentos)\n",
    "results = evaluate_models(models, data, 1, 1)\n",
    "# Imprimimos el nombre del dataset a evaluar\n",
    "print(\"Dataset name: \", dataset_name)\n",
    "# Imprimimos los promedios con el nombre de cada modelo\n",
    "for i, model in enumerate(models):\n",
    "    print(model.__class__.__name__,\n",
    "            \"Accuracy: {:.4f}\".format(results[i][0]),\n",
    "            \"Time: {:.4f}\".format(results[i][1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para evaluar todos modelos en todos los dataset e imprimir sus resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para evaluar todos modelos en todos los dataset e imprimir sus resultados promedio\n",
    "def full_models_results(num_epochs = 10, num_trials = 5):\n",
    "    # Evaluamos cada dataset\n",
    "    for di in range(3):\n",
    "        # Elegimos el dataset 0, 1 o 2 para Cora, Citeseer o Pubmed respectivamente\n",
    "        dataset, data_loader, data, dataset_name, models = set_data_nodes(di)\n",
    "        # Evaluamos los modelos\n",
    "        results = evaluate_models(models, data, num_epochs, num_trials)\n",
    "        # Imprimimos el dataset a evaluar\n",
    "        print(\"Dataset name: \", dataset_name)\n",
    "        # Imprimimos los promedios con el nombre de cada modelo\n",
    "        for i, model in enumerate(models):\n",
    "            print(model.__class__.__name__,\n",
    "                    \"Accuracy: {:.4f}\".format(results[i][0]),\n",
    "                    \"Time: {:.4f}\".format(results[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name:  Cora\n",
      "FFNN_1HL Accuracy: 0.4786 Time: 0.1733\n",
      "GNN_1HL_GraphConv Accuracy: 0.7586 Time: 0.6252\n",
      "GNN_2HL_GraphConv Accuracy: 0.7378 Time: 0.7243\n",
      "GNN_1HL_SAGEConv Accuracy: 0.7698 Time: 0.6705\n",
      "GNN_2HL_SAGEConv Accuracy: 0.7822 Time: 0.8001\n",
      "Dataset name:  Citeseer\n",
      "FFNN_1HL Accuracy: 0.5076 Time: 0.4645\n",
      "GNN_1HL_GraphConv Accuracy: 0.6126 Time: 1.5416\n",
      "GNN_2HL_GraphConv Accuracy: 0.5958 Time: 1.6608\n",
      "GNN_1HL_SAGEConv Accuracy: 0.6564 Time: 1.6232\n",
      "GNN_2HL_SAGEConv Accuracy: 0.6432 Time: 1.7810\n",
      "Dataset name:  Pubmed\n",
      "FFNN_1HL Accuracy: 0.4780 Time: 0.5642\n",
      "GNN_1HL_GraphConv Accuracy: 0.7508 Time: 2.3166\n",
      "GNN_2HL_GraphConv Accuracy: 0.7222 Time: 3.1818\n",
      "GNN_1HL_SAGEConv Accuracy: 0.6718 Time: 2.4564\n",
      "GNN_2HL_SAGEConv Accuracy: 0.7224 Time: 3.3296\n"
     ]
    }
   ],
   "source": [
    "# Imprimimos los resultados promedio de cada modelo para cada dataset\n",
    "full_models_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discusión de Resultados\n",
    "\n",
    "Respecto a los resultados podemos decir que, considerando los parámetros dados, las redes neuronales de grafos (GNN) son mejores en términos de accuracy que las redes neuronales feed-forward (FFNN) para resolver tareas en grafos como los datasets dados.\n",
    "\n",
    "Además, se puede observar que aumentar el número de capas escondidas en las GNNs no siempre mejora el accuracy, y que el tipo de agregación utilizada también puede afectar el desempeño del modelo. También se puede ver que las GNNs tardan más en ejecutarse que las FFNNs debido a la complejidad adicional que implica el tratar con datos en forma de grafos y calcular agregaciones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28df6a81e6d56781468ca2a57cef730a512fcff4b2f0b3c589a934e7dd931cbb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
